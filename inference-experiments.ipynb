{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Test inference on our trained model\n",
    "\n",
    "This was originally a notebook on Kaggle, can't promise it works on Colab\n",
    "due to different virtual environments"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## pip install and import statements ",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# according to hf modelcard need transformers >= 4.43.0 onward for Llama 3.1\n!pip install -q transformers==4.43.1",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T15:13:11.587555Z",
     "iopub.execute_input": "2024-09-02T15:13:11.588437Z",
     "iopub.status.idle": "2024-09-02T15:13:38.413925Z",
     "shell.execute_reply.started": "2024-09-02T15:13:11.588390Z",
     "shell.execute_reply": "2024-09-02T15:13:38.412676Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "!pip install -q accelerate bitsandbytes peft flash-attn",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T15:13:38.416319Z",
     "iopub.execute_input": "2024-09-02T15:13:38.416772Z",
     "iopub.status.idle": "2024-09-02T15:14:18.838747Z",
     "shell.execute_reply.started": "2024-09-02T15:13:38.416722Z",
     "shell.execute_reply": "2024-09-02T15:14:18.837501Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import huggingface_hub\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2024-09-02T15:14:18.840384Z",
     "iopub.execute_input": "2024-09-02T15:14:18.840810Z",
     "iopub.status.idle": "2024-09-02T15:14:36.676433Z",
     "shell.execute_reply.started": "2024-09-02T15:14:18.840761Z",
     "shell.execute_reply": "2024-09-02T15:14:36.675530Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Login in to Huggingface",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "huggingface_hub.login()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T15:14:36.678644Z",
     "iopub.execute_input": "2024-09-02T15:14:36.679731Z",
     "iopub.status.idle": "2024-09-02T15:14:36.708109Z",
     "shell.execute_reply.started": "2024-09-02T15:14:36.679683Z",
     "shell.execute_reply": "2024-09-02T15:14:36.707191Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7c01e66f812743eaa111ac5b13ec7055"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Define which hf things to download",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "dataset_name = \"LLMsForHepth/arxiv_hepth_first_overfit\"  # 5 abstracts\n",
    "# dataset_name = \"LLMsForHepth/arxiv_hepth_first\"  # 15,825 abstracts\n",
    "model_name = \"LLMsForHepth/test_llama_3.1_batch48\""
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T15:14:48.892751Z",
     "iopub.execute_input": "2024-09-02T15:14:48.893110Z",
     "iopub.status.idle": "2024-09-02T15:14:48.897282Z",
     "shell.execute_reply.started": "2024-09-02T15:14:48.893078Z",
     "shell.execute_reply": "2024-09-02T15:14:48.896272Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Model",
   "metadata": {}
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Notes:\n",
    "- Running inference with a quanitzed model usually gives slightly poorer quality results.\n",
    "- Inference using a PeftModel incurs an overhead as extra operations are needed\n",
    "to add the adapters "
   ]
  },
  {
   "cell_type": "code",
   "source": "bnb_config = BitsAndBytesConfig(load_in_4bit=True,\n                                bnb_4bit_compute_dtype=torch.bfloat16,\n                                bnb_4bit_quant_storage=None, #torch.bfloat16,\n                                bnb_4bit_quant_type=\"nf4\",\n                                bnb_4bit_use_double_quant=True)\n\n\nmodel_cfg = {\"attn_implementation\": \"sdpa\",  # can be \"eager\" (default),\"sdpa\" or \"flash_attention_2\"\n             \"device_map\": \"auto\",\n             \"quantization_config\": bnb_config,\n            }\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name, **model_cfg)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T15:14:50.024782Z",
     "iopub.execute_input": "2024-09-02T15:14:50.025183Z",
     "iopub.status.idle": "2024-09-02T15:21:04.045724Z",
     "shell.execute_reply.started": "2024-09-02T15:14:50.025144Z",
     "shell.execute_reply": "2024-09-02T15:21:04.044655Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "adapter_config.json:   0%|          | 0.00/730 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b0d75ea88cd842bb9e94a44d7729259d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/826 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "29b9183910bb464f9fef73d62801ec65"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3c556399ee3440e5b5e1e4a8577b4fb5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d30ee2505374420abcd7a1c14277f587"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "097b5e5b3079433ea25764ae5c515e2c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b490b5cb91d84e598731ab01dc8b12d2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5378cb04c60740aaa71a16e66e870aba"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "15c87dba5e6045c2a7a5c231b297eb4b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5e82a0a8ee95410a999e6ba0c18b521b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8c2920438e8d4e57919b7401e6c5c73b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "adapter_model.safetensors:   0%|          | 0.00/83.9M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "659893c8dca24a92b73f4fae08983e1f"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Tokenizer",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side=\"left\"  # left pad for inference"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T15:21:04.047828Z",
     "iopub.execute_input": "2024-09-02T15:21:04.048179Z",
     "iopub.status.idle": "2024-09-02T15:21:07.277870Z",
     "shell.execute_reply.started": "2024-09-02T15:21:04.048143Z",
     "shell.execute_reply": "2024-09-02T15:21:07.276980Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ddfa67104a414f6eafc11a63d2ab1a5c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2a88ca003e9f4428a43e01e1b63e023e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/335 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c8dc94c777fd43a98b072ae7a4c6746f"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Get datasets",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "ds = load_dataset(dataset_name, split='train')  # use test for dataset arxiv_hepth_first",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T15:21:07.279322Z",
     "iopub.execute_input": "2024-09-02T15:21:07.279665Z",
     "iopub.status.idle": "2024-09-02T15:21:12.469376Z",
     "shell.execute_reply.started": "2024-09-02T15:21:07.279630Z",
     "shell.execute_reply": "2024-09-02T15:21:12.468626Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading readme:   0%|          | 0.00/3.80k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8d8b76cfbca548ac971c6df3afd376f7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/18.3k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d044f449ec5f4363a53a4f02e3b21d36"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a94af7182cfd4adfaa94567cf4e86ad3"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def split_abstracts(example):\n",
    "    \"\"\"\n",
    "    Splits an abstract into a prompt and ground truth.\n",
    "\n",
    "    The prompt is created from the first half (or slightly more) of the sentences in the abstract,\n",
    "    and the ground truth is the remaining sentences.\n",
    "\n",
    "    Args:\n",
    "        example (dict): A dictionary containing the 'abstract' text to be split.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with 'prompt' and 'y_true' keys containing the split abstract parts.\n",
    "    \"\"\"\n",
    "    # Split the abstract into sentences (i.e. text sequences which end with any of .!? and a space)\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', example['abstract'])\n",
    "    # Calculate the split point\n",
    "    total_sentences = len(sentences)\n",
    "    split_point = (total_sentences + 1) // 2  # Ensures the prompt has >= number of sentences than y_true\n",
    "    # Join the sentences back into two parts\n",
    "    prompt = ' '.join(sentences[:split_point])\n",
    "    y_true = ' '.join(sentences[split_point:])\n",
    "    return {'prompt': prompt, 'y_true': y_true}"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T15:21:12.471574Z",
     "iopub.execute_input": "2024-09-02T15:21:12.472449Z",
     "iopub.status.idle": "2024-09-02T15:21:12.477895Z",
     "shell.execute_reply.started": "2024-09-02T15:21:12.472396Z",
     "shell.execute_reply": "2024-09-02T15:21:12.477067Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "ds = ds.map(split_abstracts, batched=False)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T15:21:12.479423Z",
     "iopub.execute_input": "2024-09-02T15:21:12.479863Z",
     "iopub.status.idle": "2024-09-02T15:21:12.531546Z",
     "shell.execute_reply.started": "2024-09-02T15:21:12.479820Z",
     "shell.execute_reply": "2024-09-02T15:21:12.530651Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Map:   0%|          | 0/5 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d8b0e0d75c42453cb6f2304e50a7937f"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "for row in ds:\n    print(len(row['abstract']), len(row['prompt']))",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T15:22:06.986051Z",
     "iopub.execute_input": "2024-09-02T15:22:06.986440Z",
     "iopub.status.idle": "2024-09-02T15:22:06.996689Z",
     "shell.execute_reply.started": "2024-09-02T15:22:06.986403Z",
     "shell.execute_reply": "2024-09-02T15:22:06.995550Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "text": "526 352\n1922 645\n679 344\n696 389\n993 539\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Set random seed just in case for reproducibility",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from transformers import set_seed\nset_seed(42)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T15:22:10.014198Z",
     "iopub.execute_input": "2024-09-02T15:22:10.014876Z",
     "iopub.status.idle": "2024-09-02T15:22:10.020470Z",
     "shell.execute_reply.started": "2024-09-02T15:22:10.014834Z",
     "shell.execute_reply": "2024-09-02T15:22:10.019437Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Inference using `model.generate`",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Set up some function definitions",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def get_batches(dataset, batch_size):\n    \"\"\"Splits a dataset into a list of batches.\"\"\"\n    return [dataset[i:i + batch_size] for i in range(0, len(dataset), batch_size)]  ",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T15:27:00.072141Z",
     "iopub.execute_input": "2024-09-02T15:27:00.072871Z",
     "iopub.status.idle": "2024-09-02T15:27:00.077646Z",
     "shell.execute_reply.started": "2024-09-02T15:27:00.072830Z",
     "shell.execute_reply": "2024-09-02T15:27:00.076649Z"
    },
    "trusted": true
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def tokenize_batch(batch):\n",
    "    \"\"\"Tokenizes a batch of prompts.\"\"\"\n",
    "    prompts = batch[\"prompt\"] \n",
    "    prompts_tok = tokenizer(\n",
    "        prompts, \n",
    "        return_tensors=\"pt\", \n",
    "        padding='longest', \n",
    "        truncation=False,\n",
    "        pad_to_multiple_of=8,\n",
    "        add_special_tokens=False\n",
    "    ).to(model.device)\n",
    "\n",
    "    return prompts_tok"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T15:35:44.549098Z",
     "iopub.execute_input": "2024-09-02T15:35:44.549499Z",
     "iopub.status.idle": "2024-09-02T15:35:44.554996Z",
     "shell.execute_reply.started": "2024-09-02T15:35:44.549466Z",
     "shell.execute_reply": "2024-09-02T15:35:44.553998Z"
    },
    "trusted": true
   },
   "execution_count": 35,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def generate_batch(batch_tok, generation_config):\n    with torch.no_grad():\n        outputs_tok = model.generate(**batch_tok, **generation_config).to(\"cpu\")\n        outputs = tokenizer.batch_decode(outputs_tok, skip_special_tokens=True)\n    return outputs",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T15:52:20.627172Z",
     "iopub.execute_input": "2024-09-02T15:52:20.627590Z",
     "iopub.status.idle": "2024-09-02T15:52:20.633158Z",
     "shell.execute_reply.started": "2024-09-02T15:52:20.627549Z",
     "shell.execute_reply": "2024-09-02T15:52:20.632256Z"
    },
    "trusted": true
   },
   "execution_count": 44,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def clear_cache():\n    torch.cuda.empty_cache()\n    gc.collect()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T16:19:19.840187Z",
     "iopub.execute_input": "2024-09-02T16:19:19.840907Z",
     "iopub.status.idle": "2024-09-02T16:19:19.845463Z",
     "shell.execute_reply.started": "2024-09-02T16:19:19.840866Z",
     "shell.execute_reply": "2024-09-02T16:19:19.844362Z"
    },
    "trusted": true
   },
   "execution_count": 54,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def parse_y_pred(example):\n",
    "    \"\"\"\n",
    "    Extracts the predicted text from the generated predictions.\n",
    "\n",
    "    Args:\n",
    "        example (dict): A dictionary containing 'prompt' and 'predictions' keys.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with 'y_pred' key containing the generated output without the prompt.\n",
    "    \"\"\"\n",
    "    len_prompt = len(example['prompt'])\n",
    "    y_pred = example['predictions'][len_prompt:]\n",
    "    return {'y_pred': y_pred}"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Get batches and tokenize them",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "batches = get_batches(ds, 8)\n",
    "batch_tok = tokenize_batch(batches[0])  # select a single batch for speedy testing"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Set up generation configurations",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# greedy search\n",
    "gen_cfg1 = {'pad_token_id': tokenizer.pad_token_id, 'max_new_tokens': 1024}\n",
    "# contrastive search\n",
    "gen_cfg2 = {'pad_token_id': tokenizer.pad_token_id, 'max_new_tokens': 1024, 'penalty_alpha': 0.6, 'top_k': 4}\n",
    "# multinomial sampling\n",
    "gen_cfg3 = {'pad_token_id': tokenizer.pad_token_id, 'max_new_tokens': 1024, 'do_sample': True, 'num_beams': 1}\n",
    "# beam-search decoding\n",
    "gen_cfg4 = {'pad_token_id': tokenizer.pad_token_id, 'max_new_tokens': 1024, 'num_beams': 5}\n",
    "# beam-search multinomial sampling\n",
    "gen_cfg5 = {'pad_token_id': tokenizer.pad_token_id, 'max_new_tokens': 1024, 'num_beams': 5, 'do_sample': True}\n",
    "# diverse beam search decoding\n",
    "gen_cfg6 = {'pad_token_id': tokenizer.pad_token_id, 'max_new_tokens': 1024, 'num_beams': 5,\n",
    "            'num_beam_groups': 5, 'diversity_penalty': 1.0, 'do_sample': False}\n",
    "\n",
    "num_gen_cfgs = 6  # how many configurations have we set up immediately above"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T17:11:24.661803Z",
     "iopub.execute_input": "2024-09-02T17:11:24.662719Z",
     "iopub.status.idle": "2024-09-02T17:11:24.669469Z",
     "shell.execute_reply.started": "2024-09-02T17:11:24.662677Z",
     "shell.execute_reply": "2024-09-02T17:11:24.668432Z"
    },
    "trusted": true
   },
   "execution_count": 69,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Return predictions using a single generation config",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "result = generate_batch(batch_tok, gen_cfg5)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T17:19:32.818722Z",
     "iopub.execute_input": "2024-09-02T17:19:32.819419Z",
     "iopub.status.idle": "2024-09-02T17:34:57.902159Z",
     "shell.execute_reply.started": "2024-09-02T17:19:32.819379Z",
     "shell.execute_reply": "2024-09-02T17:34:57.901252Z"
    },
    "trusted": true
   },
   "execution_count": 75,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Iterate through all the generation configs and return a list of dictionaries\n",
    "\n",
    "Currently only uses a single batch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "results = []\n",
    "for idx in tqdm(range(1,1+num_gen_cfgs)):\n",
    "    result_dict = {}\n",
    "    gen_cfg = eval('gen_cfg' + str(idx))\n",
    "    result = generate_batch(batch_tok, gen_cfg)\n",
    "    result_dict.update({**gen_cfg, 'predictions': result})\n",
    "    results.append(result_dict)\n",
    "    clear_cache()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T18:04:28.945100Z",
     "iopub.execute_input": "2024-09-02T18:04:28.945810Z",
     "iopub.status.idle": "2024-09-02T18:56:44.331943Z",
     "shell.execute_reply.started": "2024-09-02T18:04:28.945771Z",
     "shell.execute_reply": "2024-09-02T18:56:44.330959Z"
    },
    "trusted": true
   },
   "execution_count": 77,
   "outputs": [
    {
     "name": "stderr",
     "text": "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [52:15<00:00, 522.56s/it]\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "import json\nfilepath = '/Users/paul/Desktop/results.json'\nwith open(filepath, 'w') as file:\n    json.dump(results, file, indent=4)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-02T18:58:21.582252Z",
     "iopub.execute_input": "2024-09-02T18:58:21.583116Z",
     "iopub.status.idle": "2024-09-02T18:58:21.589815Z",
     "shell.execute_reply.started": "2024-09-02T18:58:21.583065Z",
     "shell.execute_reply": "2024-09-02T18:58:21.588748Z"
    },
    "trusted": true
   },
   "execution_count": 79,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Ideas for inference speedup\n\n* Use unsloth - limited to single gpu but claim 2x inference speed up\n* Try using torch.compile(model)\n* Use accelerate for multiple gpu use",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
