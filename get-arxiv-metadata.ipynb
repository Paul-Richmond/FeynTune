{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9193845,"sourceType":"datasetVersion","datasetId":612177,"isSourceIdPinned":true}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Creation of datasets for finetuning LLMs on arXiv abstracts\n\nUseful info can be found here: https://info.arxiv.org/help/arxiv_identifier_for_services.html\n\n# Outline\n\n- [ 1 - Packages and setup](#1)\n    - [1.1 - Log into huggingface hub](#1.1)\n    - [1.2 - Define variables to automate extraction and upload](#1.2)\n- [ 2 - Load full arXiv metadata (currently ~4.2Gb)](#2)\n- [ 3 - Data manipulation](#3)\n    - [3.1 - Identify small set of papers which we authored](#3.1)\n    - [3.2 - Explore categories](#3.2)\n    - [3.3 - Extract specific category/categories](#3.3)\n    - [3.4 - Remove papers that have been withdrawn](#3.4)\n- [ 4 - Look at the abstracts](#4)    \n    - [4.1 - Length of abstracts](#4.1)\n    - [4.2 - Keywords/PACS at end of abstracts](#4.2)\n    - [4.3 - Multi-lingual abstracts](#4.3)\n    - [4.4 - Look at distribution of dates from `id` column](#4.4)\n- [ 5 - Clean the abstract data](#5)\n- [ 6 - Convert to Huggingface dataset and push](#6)\n    - [ 6.1 - Convert Pandas DataFrame to dataset Dataset](#6.1)\n    - [ 6.2 - Split dataset into train, test and validation datasets](#6.2)\n    - [ 6.3 - Upload data to Huggingface](#6.3)\n- [ 7 - Concatenate hep-th_primary and hep-ph_gr-qc_primary datasets](#7)","metadata":{}},{"cell_type":"markdown","source":"<a name=\"1\"></a>\n## 1 - Packages and setup","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport json\nimport re\n\nimport huggingface_hub\nimport datasets","metadata":{"execution":{"iopub.status.busy":"2024-10-31T11:55:02.042945Z","iopub.execute_input":"2024-10-31T11:55:02.043431Z","iopub.status.idle":"2024-10-31T11:55:04.771216Z","shell.execute_reply.started":"2024-10-31T11:55:02.043396Z","shell.execute_reply":"2024-10-31T11:55:04.770114Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"<a name=\"1.1\"></a>\n### 1.1 - Log into huggingface hub","metadata":{}},{"cell_type":"code","source":"try:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    secret_value_0 = user_secrets.get_secret(\"HFapi\")\n    huggingface_hub.login(secret_value_0)\nexcept:\n    huggingface_hub.login()","metadata":{"execution":{"iopub.status.busy":"2024-10-31T11:55:04.773428Z","iopub.execute_input":"2024-10-31T11:55:04.774102Z","iopub.status.idle":"2024-10-31T11:55:05.120226Z","shell.execute_reply.started":"2024-10-31T11:55:04.774043Z","shell.execute_reply":"2024-10-31T11:55:05.119001Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name=\"1.2\"></a>\n### 1.2 - Define variables to automate extraction and upload\n\nIn this subsection we define several variables which control how this notebook operates. Once these variables are set you can click `Run All`.\n\nThe variables are\n1. `repo_id`: String. This is the name of the Huggingface repository that the dataset will be upload to. If the repository does not already exist it will be created.\n2. `commit_message`: String or None. An optional commit message used in the `push_to_hub` function. Set it to `None` for an initial commit.\n3. `wanted_categories`: List. A list of strings corresponding to arXiv categories. The code looks at the string data in the `categories` column and checks if any of the entries in `wanted_categories` appears.\n4. `primary_classification_only`: Boolean. \n    * `True` will only match entries in `wanted_categories` to the first substring which appears in the `categories` column. \n    * `False` will match entries in `wanted_categories` if they appear *anywhere* in the `categories` column.\n5. `train_size`: Float. Represents a percentage of data to use in creating a training dataset. `(1-train_size)` is used as a non-train dataset *i.e.* combined test and validation set.\n6. `validation_size`: Float. Represents a percentage of data from the non-train dataset to use as a validation dataset. The remaining percentage, `(1-validation_size)`, is used as a test dataset. The test set is to be used for hyperparameter tuning etc with the validation set left unused until the end to determine final model performance.","metadata":{}},{"cell_type":"code","source":"repo_id = \"LLMsForHepth/hep-th_primary\"\ncommit_message = None # None will use default in `push_to_hub` which is `\"Upload dataset\"`\n\nwanted_categories = ['hep-th'] \nprimary_classification_only = True\n\ntrain_size = 0.7 # use 70% of the dataset for training, 30% for testing & validation\nvalidation_size = 0.5 # test_size is 1 - validation_size","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:52:47.269780Z","iopub.execute_input":"2024-09-21T15:52:47.270229Z","iopub.status.idle":"2024-09-21T15:52:47.277348Z","shell.execute_reply.started":"2024-09-21T15:52:47.270189Z","shell.execute_reply":"2024-09-21T15:52:47.276083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name=\"2\"></a>\n## 2 - Load full arXiv metadata (currently ~4.2Gb)\n\nThe Kaggle dataset is described here: https://www.kaggle.com/datasets/Cornell-University/arxiv\n\nIn this notebook we've pinned the dataset to be v193 which includes submissions upto around 22nd August 2024.\n\n\nArxiv submissions are tightly controlled and should follow the instructions given here https://info.arxiv.org/help/prep.html.\n\nIn particular, the Title and Abstract metadata must be in ASCII input and Unicode characters should be converted to LaTex equivalent.\nSince ASCII is a subset of utf-8 we can use utf-8 encoding to parse the json file.","metadata":{}},{"cell_type":"code","source":"df_dir='/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json'\njson_data = []\n\nwith open(df_dir, 'r', encoding='utf-8') as f:\n    for line in f:\n        # Parse JSON from each line\n        try:\n            json_object = json.loads(line)\n            json_data.append(json_object)\n        except json.JSONDecodeError as e:\n            print(f\"Error decoding JSON: {e}\")\n            continue\n            \ndf = pd.DataFrame(json_data)\ndel json_data","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:52:47.279706Z","iopub.execute_input":"2024-09-21T15:52:47.280056Z","iopub.status.idle":"2024-09-21T15:55:07.224418Z","shell.execute_reply.started":"2024-09-21T15:52:47.280025Z","shell.execute_reply":"2024-09-21T15:55:07.223336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name=\"3\"></a>\n## 3 - Data manipulation","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:55:07.228333Z","iopub.execute_input":"2024-09-21T15:55:07.228721Z","iopub.status.idle":"2024-09-21T15:55:07.250431Z","shell.execute_reply.started":"2024-09-21T15:55:07.228663Z","shell.execute_reply":"2024-09-21T15:55:07.249286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:55:07.253990Z","iopub.execute_input":"2024-09-21T15:55:07.254859Z","iopub.status.idle":"2024-09-21T15:55:07.459544Z","shell.execute_reply.started":"2024-09-21T15:55:07.254825Z","shell.execute_reply":"2024-09-21T15:55:07.458052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name=\"3.1\"></a>\n### 3.1 - Identify small set of papers which we authored\n\nWe want to create a very small control dataset so we can see how well a LLM completes abstracts as it is being finetuned.\n\nWe choose the id of the paper which appears as the $n$th entry in each of our inspires record (Sid not included as no hep-th papers)\nwith $n = \\text{entry number of citeable papers} \\, // \\, 2$.","metadata":{}},{"cell_type":"code","source":"ids = [\"1804.08625\", \"1404.0016\", \"1205.2086\", \"1209.5915\", \"1802.05268\"]\ndf_overfit = df[df['id'].isin(ids)]\ndf_overfit","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:55:07.460663Z","iopub.execute_input":"2024-09-21T15:55:07.461016Z","iopub.status.idle":"2024-09-21T15:55:07.992482Z","shell.execute_reply.started":"2024-09-21T15:55:07.460989Z","shell.execute_reply":"2024-09-21T15:55:07.991428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove our papers from main dataframe so we don't double count\ndf = df[~df['id'].isin(ids)]","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:55:07.993973Z","iopub.execute_input":"2024-09-21T15:55:07.994282Z","iopub.status.idle":"2024-09-21T15:55:09.518164Z","shell.execute_reply.started":"2024-09-21T15:55:07.994256Z","shell.execute_reply":"2024-09-21T15:55:09.517177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name=\"3.2\"></a>\n### 3.2 - Explore categories\n\nSee https://arxiv.org/category_taxonomy for a desciption of the values which can appear.\nEach arXiv article has a primary category and may also have one or more cross-lists to other categories.\n\nInspecting `df.head()` we see that `df.iloc[1]['categories'] = 'math.CO cs.CG'`. The *primary* classifcation is `math.CO` and it is also cross-listed to the `cs.BG` category.","metadata":{}},{"cell_type":"code","source":"# there are many combinations of primary and cross-list categories\ndf['categories'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:55:09.519694Z","iopub.execute_input":"2024-09-21T15:55:09.520447Z","iopub.status.idle":"2024-09-21T15:55:10.297249Z","shell.execute_reply.started":"2024-09-21T15:55:09.520403Z","shell.execute_reply":"2024-09-21T15:55:10.296223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split string appearing in `categories` on white space and expand\n# split_cats[0] is the *primary* classification\nsplit_cats = df['categories'].str.split(n=-1, expand=True)\nprimary_cat = split_cats[0]","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:55:10.300565Z","iopub.execute_input":"2024-09-21T15:55:10.300895Z","iopub.status.idle":"2024-09-21T15:55:23.474578Z","shell.execute_reply.started":"2024-09-21T15:55:10.300868Z","shell.execute_reply":"2024-09-21T15:55:23.473411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get a list of primary classifications and associated count\n# list is ordered in descending count  \nprimary_cats_and_counts = list(zip(split_cats[0].value_counts().keys().tolist(), split_cats[0].value_counts().tolist()))\nprimary_cats_and_counts","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:55:23.475971Z","iopub.execute_input":"2024-09-21T15:55:23.476332Z","iopub.status.idle":"2024-09-21T15:55:24.503994Z","shell.execute_reply.started":"2024-09-21T15:55:23.476299Z","shell.execute_reply":"2024-09-21T15:55:24.502590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name=\"3.3\"></a>\n### 3.3 - Extract specific category/categories\n\n**NB:** The variables `wanted_categories` and `primary_classification_only` are defined in section [1.2](#1.2)","metadata":{}},{"cell_type":"code","source":"if primary_classification_only:\n    # we get those papers whose *primary* classification is in `wanted_categories`\n    df = df[primary_cat.apply(lambda x: any(k in x for k in wanted_categories))]\nelse:\n    # we get papers where `wanted_categories` appears anywhere in `categories` i.e. primary and also in cross-listing\n    df = df[df['categories'].apply(lambda x: any(k in x for k in wanted_categories))]","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:55:24.505311Z","iopub.execute_input":"2024-09-21T15:55:24.505695Z","iopub.status.idle":"2024-09-21T15:55:27.013285Z","shell.execute_reply.started":"2024-09-21T15:55:24.505664Z","shell.execute_reply":"2024-09-21T15:55:27.012219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:55:27.014538Z","iopub.execute_input":"2024-09-21T15:55:27.014883Z","iopub.status.idle":"2024-09-21T15:55:27.020770Z","shell.execute_reply.started":"2024-09-21T15:55:27.014853Z","shell.execute_reply":"2024-09-21T15:55:27.019835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name=\"3.4\"></a>\n### 3.4 - Remove papers that have been withdrawn\n\nSee https://info.arxiv.org/help/withdraw.html","metadata":{}},{"cell_type":"code","source":"# make an index of comments which contain either 'Withdrawn' or 'withdrawn'\nwithdrawn = df['comments'].str.contains('Withdrawn', case=False) # empty comments return None\nwithdrawn.fillna(value=False, inplace=True) # replace None with False\nwithdrawn.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:55:27.021882Z","iopub.execute_input":"2024-09-21T15:55:27.022170Z","iopub.status.idle":"2024-09-21T15:55:27.201374Z","shell.execute_reply.started":"2024-09-21T15:55:27.022146Z","shell.execute_reply":"2024-09-21T15:55:27.200242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sanity check but takes a while\n# make an index of abstracts which contain either 'Withdrawn' or 'withdrawn'\n# this way is quicker than using contains('withdrawn', case=False)\nwithdrawn_abs = df['abstract'].str.contains('Withdrawn') | df['abstract'].str.contains('withdrawn') #| df['abstract'].str.contains('removed') \nwithdrawn_abs.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:55:27.202954Z","iopub.execute_input":"2024-09-21T15:55:27.203370Z","iopub.status.idle":"2024-09-21T15:55:27.448977Z","shell.execute_reply.started":"2024-09-21T15:55:27.203335Z","shell.execute_reply":"2024-09-21T15:55:27.447934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# look at entries with `withdrawn` in abstract but not in comments\ndf[~withdrawn & withdrawn_abs]","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:55:27.450129Z","iopub.execute_input":"2024-09-21T15:55:27.450421Z","iopub.status.idle":"2024-09-21T15:55:27.498821Z","shell.execute_reply.started":"2024-09-21T15:55:27.450396Z","shell.execute_reply":"2024-09-21T15:55:27.497872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop the withdrawn papers\ndf = df[~(withdrawn | withdrawn_abs)]\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:55:27.500176Z","iopub.execute_input":"2024-09-21T15:55:27.500489Z","iopub.status.idle":"2024-09-21T15:55:27.573077Z","shell.execute_reply.started":"2024-09-21T15:55:27.500463Z","shell.execute_reply":"2024-09-21T15:55:27.572013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name=\"4\"></a>\n## 4 - Look at the abstracts","metadata":{}},{"cell_type":"markdown","source":"<a name=\"4.1\"></a>\n### 4.1 - Length of abstracts","metadata":{}},{"cell_type":"code","source":"df.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:55:27.574351Z","iopub.execute_input":"2024-09-21T15:55:27.574707Z","iopub.status.idle":"2024-09-21T15:55:27.579252Z","shell.execute_reply.started":"2024-09-21T15:55:27.574658Z","shell.execute_reply":"2024-09-21T15:55:27.578247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the number of characters in each abstract\nabstract_len = df['abstract'].map(lambda x: len(x))\n# look at the summary statistics for abstract_len\nabstract_len.describe()\n# According to https://info.arxiv.org/help/prep.html\n# abstracts longer than 1920 characters are not accepted. \n# So when did this rule begin as we have examples of abstract_len > 1920?","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:55:27.580587Z","iopub.execute_input":"2024-09-21T15:55:27.580903Z","iopub.status.idle":"2024-09-21T15:55:27.663079Z","shell.execute_reply.started":"2024-09-21T15:55:27.580873Z","shell.execute_reply":"2024-09-21T15:55:27.662083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"abstract_len.hist()","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:55:27.664291Z","iopub.execute_input":"2024-09-21T15:55:27.664622Z","iopub.status.idle":"2024-09-21T15:55:28.002132Z","shell.execute_reply.started":"2024-09-21T15:55:27.664595Z","shell.execute_reply":"2024-09-21T15:55:28.000977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Take a look at the longest abstract\ndf.iloc[abstract_len.idxmax()]['abstract']","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:55:28.003400Z","iopub.execute_input":"2024-09-21T15:55:28.003873Z","iopub.status.idle":"2024-09-21T15:55:28.013002Z","shell.execute_reply.started":"2024-09-21T15:55:28.003840Z","shell.execute_reply":"2024-09-21T15:55:28.011908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Things to notice about the above abstract\n- there is lots of whitespace at the start\n- there are many \\n instead of spaces\n\nThis suggests we replace \\n with ' ' and strip out the extra leading/trailing whitespaces.","metadata":{}},{"cell_type":"markdown","source":"<a name=\"4.2\"></a>\n### 4.2 - Keywords/PACS at end of abstracts","metadata":{}},{"cell_type":"code","source":"# Turns out there's PACS numbers and Keywords at the end of some abstracts,\n# should we remove these for training a LLM?\n# Find which abstracts contain either 'Keyword' or 'PACS'\nhas_keyword = df['abstract'].str.contains('Keyword|PACS', case=False)\ndf[has_keyword].shape[0]","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:55:28.014184Z","iopub.execute_input":"2024-09-21T15:55:28.015275Z","iopub.status.idle":"2024-09-21T15:55:30.942052Z","shell.execute_reply.started":"2024-09-21T15:55:28.015243Z","shell.execute_reply":"2024-09-21T15:55:30.940876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df[has_keyword].iloc[0]['abstract']","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:55:30.943375Z","iopub.execute_input":"2024-09-21T15:55:30.943795Z","iopub.status.idle":"2024-09-21T15:55:30.948677Z","shell.execute_reply.started":"2024-09-21T15:55:30.943759Z","shell.execute_reply":"2024-09-21T15:55:30.947492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Percentage of abstacts with Keywords or PACS is {100 * df[has_keyword].shape[0] / df.shape[0]:.3f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:55:30.950128Z","iopub.execute_input":"2024-09-21T15:55:30.950560Z","iopub.status.idle":"2024-09-21T15:55:30.964808Z","shell.execute_reply.started":"2024-09-21T15:55:30.950521Z","shell.execute_reply":"2024-09-21T15:55:30.963698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name=\"4.3\"></a>\n### 4.3 - Multi-lingual abstracts\n\nSee https://info.arxiv.org/help/faq/multilang.html","metadata":{}},{"cell_type":"markdown","source":"First we find any multi-lingual abstracts","metadata":{}},{"cell_type":"code","source":"multi = df['abstract'].str.contains(\"-----\")\nmulti.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:55:30.966187Z","iopub.execute_input":"2024-09-21T15:55:30.966569Z","iopub.status.idle":"2024-09-21T15:55:31.092993Z","shell.execute_reply.started":"2024-09-21T15:55:30.966533Z","shell.execute_reply":"2024-09-21T15:55:31.091953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Percentage of multi-lingual abstacts is {100 * df[multi].shape[0] / df.shape[0]:.3f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:55:31.094158Z","iopub.execute_input":"2024-09-21T15:55:31.094454Z","iopub.status.idle":"2024-09-21T15:55:31.101049Z","shell.execute_reply.started":"2024-09-21T15:55:31.094430Z","shell.execute_reply":"2024-09-21T15:55:31.099781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"english_only = df['abstract'].apply(lambda x: x.split(\"-----\")[0])","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:55:31.102301Z","iopub.execute_input":"2024-09-21T15:55:31.102648Z","iopub.status.idle":"2024-09-21T15:55:31.195586Z","shell.execute_reply.started":"2024-09-21T15:55:31.102617Z","shell.execute_reply":"2024-09-21T15:55:31.194449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have to be careful because there are some abstracts which have metric signatures denoted by $+-----$ as can be seen below!","metadata":{}},{"cell_type":"code","source":"# df[multi]['abstract'].iloc[3]","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:55:31.201774Z","iopub.execute_input":"2024-09-21T15:55:31.202236Z","iopub.status.idle":"2024-09-21T15:55:31.206430Z","shell.execute_reply.started":"2024-09-21T15:55:31.202204Z","shell.execute_reply":"2024-09-21T15:55:31.205285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To remove the \"-----\" and everything after it we would use the following","metadata":{}},{"cell_type":"code","source":"english_only = df['abstract'].apply(lambda x: x.split(\"-----\")[0])","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:55:31.208110Z","iopub.execute_input":"2024-09-21T15:55:31.208560Z","iopub.status.idle":"2024-09-21T15:55:31.304431Z","shell.execute_reply.started":"2024-09-21T15:55:31.208498Z","shell.execute_reply":"2024-09-21T15:55:31.303231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"However, since there are very few examples we leave things as they are","metadata":{}},{"cell_type":"markdown","source":"## 4.4 - Look at distribution of dates from `id` column\n\nOld scheme identifiers are of the form hep-th/9901001.\n\nNew scheme identifiers are of the form 0704.0001 or 1501.00001","metadata":{}},{"cell_type":"code","source":"# The dataframes look to be ordered by identifier\ndf['id'][:-10]","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:55:31.305747Z","iopub.execute_input":"2024-09-21T15:55:31.306082Z","iopub.status.idle":"2024-09-21T15:55:31.315148Z","shell.execute_reply.started":"2024-09-21T15:55:31.306053Z","shell.execute_reply":"2024-09-21T15:55:31.313959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_year_from_id(id):\n    if '.' in id:\n        year = id[:2]\n    else:\n        tmp = id.split('/')[1]\n        year = tmp[:2]\n    if year[0] == '9':\n        year = '19' + year\n    else:\n        year = '20' + year\n    return year","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:55:31.316414Z","iopub.execute_input":"2024-09-21T15:55:31.316792Z","iopub.status.idle":"2024-09-21T15:55:31.324662Z","shell.execute_reply.started":"2024-09-21T15:55:31.316748Z","shell.execute_reply":"2024-09-21T15:55:31.323743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"years = df['id'].map(get_year_from_id)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:55:31.326023Z","iopub.execute_input":"2024-09-21T15:55:31.326802Z","iopub.status.idle":"2024-09-21T15:55:31.419615Z","shell.execute_reply.started":"2024-09-21T15:55:31.326763Z","shell.execute_reply":"2024-09-21T15:55:31.418560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"years.sort_values(ascending=True).hist(figsize=(15,3))","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:55:31.420810Z","iopub.execute_input":"2024-09-21T15:55:31.421151Z","iopub.status.idle":"2024-09-21T15:55:32.037030Z","shell.execute_reply.started":"2024-09-21T15:55:31.421123Z","shell.execute_reply":"2024-09-21T15:55:32.035996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name=\"5\"></a>\n## 5 - Clean the abstract data","metadata":{}},{"cell_type":"code","source":"def clean_abstracts(abstract):\n    abstract = re.sub(r'\\n\\s*', ' ', abstract)  # replace '\\n' and any whitespace immediately after it with a single whitespace\n    abstract = abstract.strip()  # remove leading/trailing whitespace\n    return abstract","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:55:32.038772Z","iopub.execute_input":"2024-09-21T15:55:32.039701Z","iopub.status.idle":"2024-09-21T15:55:32.046136Z","shell.execute_reply.started":"2024-09-21T15:55:32.039659Z","shell.execute_reply":"2024-09-21T15:55:32.044489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# apply `clean_abstracts` function to Series. Don't know how to do this inplace\n# so we add a new column and then do some renaming\ndf['cleaned_abstract'] = df['abstract'].map(clean_abstracts)\ndf = df.rename(columns={\"abstract\": \"orig_abstract\", \"cleaned_abstract\": \"abstract\"})","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:55:32.047867Z","iopub.execute_input":"2024-09-21T15:55:32.048287Z","iopub.status.idle":"2024-09-21T15:55:32.837102Z","shell.execute_reply.started":"2024-09-21T15:55:32.048254Z","shell.execute_reply":"2024-09-21T15:55:32.836123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['abstract'].iloc[0]","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:55:32.838314Z","iopub.execute_input":"2024-09-21T15:55:32.838655Z","iopub.status.idle":"2024-09-21T15:55:32.844907Z","shell.execute_reply.started":"2024-09-21T15:55:32.838627Z","shell.execute_reply":"2024-09-21T15:55:32.843941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name=\"6\"></a>\n## 6 - Convert to Huggingface dataset and push","metadata":{}},{"cell_type":"markdown","source":"<a name=\"6.1\"></a>\n### 6.1 - Convert Pandas DataFrame to dataset Dataset","metadata":{}},{"cell_type":"code","source":"raw_dataset = datasets.Dataset.from_pandas(df, preserve_index=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:55:32.846090Z","iopub.execute_input":"2024-09-21T15:55:32.846372Z","iopub.status.idle":"2024-09-21T15:55:35.217048Z","shell.execute_reply.started":"2024-09-21T15:55:32.846349Z","shell.execute_reply":"2024-09-21T15:55:35.215989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name=\"6.2\"></a>\n### 6.2 - Split dataset into train, test and validation datasets\n\nuse `?datasets.Dataset.train_test_split` to get full documentation.\n\nSince the DataFrame seems to be ordered by `id` column we must randomly shuffle before splitting.\n\n**NB:** The variables `train_size` and `validation_size` are defined in section [1.2](#1.2)","metadata":{}},{"cell_type":"code","source":"train_testvalid = raw_dataset.train_test_split(train_size=train_size, seed=42, shuffle=True)\ntest_valid = train_testvalid['test'].train_test_split(test_size=validation_size, seed=42, shuffle=True)\n\ntrain_test_valid_dataset = datasets.DatasetDict({'train': train_testvalid['train'],\n                                                 'test': test_valid['test'],\n                                                 'validation': test_valid['train']})","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:55:35.218163Z","iopub.execute_input":"2024-09-21T15:55:35.218465Z","iopub.status.idle":"2024-09-21T15:55:35.286489Z","shell.execute_reply.started":"2024-09-21T15:55:35.218440Z","shell.execute_reply":"2024-09-21T15:55:35.285450Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print the number of entries in each dataset\nfor name, data in train_test_valid_dataset.items():\n    print(f\"Dataset {name} has size {data.shape[0]}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:55:35.287701Z","iopub.execute_input":"2024-09-21T15:55:35.288015Z","iopub.status.idle":"2024-09-21T15:55:35.293590Z","shell.execute_reply.started":"2024-09-21T15:55:35.287986Z","shell.execute_reply":"2024-09-21T15:55:35.292565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name=\"6.3\"></a>\n### 6.3 - Upload data to Huggingface\n\nSee `?datasets.Dataset.push_to_hub` for full documentation.\n\n**NB:** The variables `repo_id` and `commit_message` are defined in section [1.2](#1.2)","metadata":{}},{"cell_type":"code","source":"# Push the Dataset to Huggingface\ntry:\n    train_test_valid_dataset.push_to_hub(repo_id, commit_message=commit_message)\nexcept:\n    huggingface_hub.create_repo(repo_id=repo_id,\n                                repo_type=\"dataset\",\n                                private=True,\n                                commit_message=commit_message)\n    train_test_valid_dataset.push_to_hub(repo_id)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T15:56:58.795831Z","iopub.execute_input":"2024-09-21T15:56:58.796268Z","iopub.status.idle":"2024-09-21T15:57:51.931789Z","shell.execute_reply.started":"2024-09-21T15:56:58.796236Z","shell.execute_reply":"2024-09-21T15:57:51.930701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# logout from Huggingface\nhuggingface_hub.logout()","metadata":{"execution":{"iopub.status.busy":"2024-09-20T12:12:44.256388Z","iopub.execute_input":"2024-09-20T12:12:44.258010Z","iopub.status.idle":"2024-09-20T12:12:44.296766Z","shell.execute_reply.started":"2024-09-20T12:12:44.257859Z","shell.execute_reply":"2024-09-20T12:12:44.291349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****NB: we can get previous instances of datasets by using****\n\n```\nds_old = datasets.load_dataset('LLMsForHepth/arxiv_hepth_first', \n                               revision='346140be7a01f109af9845a0e3742b9fcd66fd9a')\n                               ```\n                               \nwhere '346140be7a01f109af9845a0e3742b9fcd66fd9a' is a commit hash found on the repo website","metadata":{"execution":{"iopub.status.busy":"2024-08-28T07:33:04.581733Z","iopub.execute_input":"2024-08-28T07:33:04.582462Z","iopub.status.idle":"2024-08-28T07:33:20.156723Z","shell.execute_reply.started":"2024-08-28T07:33:04.582423Z","shell.execute_reply":"2024-08-28T07:33:20.155819Z"}}},{"cell_type":"code","source":"ds = load_dataset('LLMsForHepth/hep-th_primary')","metadata":{"execution":{"iopub.status.busy":"2024-09-23T13:32:27.262700Z","iopub.execute_input":"2024-09-23T13:32:27.263594Z","iopub.status.idle":"2024-09-23T13:32:49.850959Z","shell.execute_reply.started":"2024-09-23T13:32:27.263558Z","shell.execute_reply":"2024-09-23T13:32:49.849757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name=\"7\"></a>\n## 7 - Concatenate hep-th_primary and hep-ph_gr-qc_primary datasets","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset, concatenate_datasets, DatasetDict","metadata":{"execution":{"iopub.status.busy":"2024-10-31T11:55:05.121619Z","iopub.execute_input":"2024-10-31T11:55:05.122102Z","iopub.status.idle":"2024-10-31T11:55:05.127450Z","shell.execute_reply.started":"2024-10-31T11:55:05.122039Z","shell.execute_reply":"2024-10-31T11:55:05.126165Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"ds_1 = load_dataset('LLMsForHepth/hep-th_primary')\nds_2 = load_dataset('LLMsForHepth/hep-ph_gr-qc_primary')","metadata":{"execution":{"iopub.status.busy":"2024-10-31T11:55:06.962301Z","iopub.execute_input":"2024-10-31T11:55:06.962741Z","iopub.status.idle":"2024-10-31T11:55:30.225822Z","shell.execute_reply.started":"2024-10-31T11:55:06.962708Z","shell.execute_reply":"2024-10-31T11:55:30.224646Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/4.72k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa7fb732fa7243d9ba626dd92e8d2ec6"}},"metadata":{}},{"name":"stderr","text":"Downloading data: 100%|██████████| 80.6M/80.6M [00:02<00:00, 36.8MB/s]\nDownloading data: 100%|██████████| 17.3M/17.3M [00:00<00:00, 28.4MB/s]\nDownloading data: 100%|██████████| 17.3M/17.3M [00:00<00:00, 26.0MB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/73768 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d8e898488ac448b9a59acc496f3593f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/15808 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48ec103a6c8f4ed58eb3cdc61824e841"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/15808 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98742245f03f4562bd2e820645e540e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/3.80k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1db06861f95f461ab33d1cc18d7a53fa"}},"metadata":{}},{"name":"stderr","text":"Downloading data: 100%|██████████| 166M/166M [00:04<00:00, 40.8MB/s] \nDownloading data: 100%|██████████| 35.6M/35.6M [00:01<00:00, 32.7MB/s]\nDownloading data: 100%|██████████| 35.5M/35.5M [00:01<00:00, 34.4MB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/137136 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4454e0c229904b23a2cb2b2042ea921c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/29387 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"838f1a2f181345ae9c51b9834fd1511d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/29386 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"158d2a5352a24c779ce9d570b19cfa4f"}},"metadata":{}}]},{"cell_type":"code","source":"ds_concat = DatasetDict()\nnames = ds_1.keys()\n\nfor name in names:\n    ds_concat[name] = concatenate_datasets([ds_1[name], ds_2[name]])","metadata":{"execution":{"iopub.status.busy":"2024-10-31T11:55:46.921821Z","iopub.execute_input":"2024-10-31T11:55:46.922421Z","iopub.status.idle":"2024-10-31T11:55:46.961709Z","shell.execute_reply.started":"2024-10-31T11:55:46.922383Z","shell.execute_reply":"2024-10-31T11:55:46.960520Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"ds_concat","metadata":{"execution":{"iopub.status.busy":"2024-10-31T11:55:54.847588Z","iopub.execute_input":"2024-10-31T11:55:54.847997Z","iopub.status.idle":"2024-10-31T11:55:54.856301Z","shell.execute_reply.started":"2024-10-31T11:55:54.847952Z","shell.execute_reply":"2024-10-31T11:55:54.855069Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi', 'report-no', 'categories', 'license', 'orig_abstract', 'versions', 'update_date', 'authors_parsed', 'abstract'],\n        num_rows: 210904\n    })\n    test: Dataset({\n        features: ['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi', 'report-no', 'categories', 'license', 'orig_abstract', 'versions', 'update_date', 'authors_parsed', 'abstract'],\n        num_rows: 45195\n    })\n    validation: Dataset({\n        features: ['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi', 'report-no', 'categories', 'license', 'orig_abstract', 'versions', 'update_date', 'authors_parsed', 'abstract'],\n        num_rows: 45194\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"ds_concat['train'] = ds_concat['train'].shuffle(seed=42)\nds_concat['train'] = ds_concat['train'].flatten_indices()\n\nds_concat['test'] = ds_concat['test'].shuffle(seed=42)\nds_concat['test'] = ds_concat['test'].flatten_indices()\n\nds_concat['validation'] = ds_concat['validation'].shuffle(seed=42)\nds_concat['validation'] = ds_concat['validation'].flatten_indices()","metadata":{"execution":{"iopub.status.busy":"2024-10-31T11:56:00.624336Z","iopub.execute_input":"2024-10-31T11:56:00.624751Z","iopub.status.idle":"2024-10-31T11:57:45.966569Z","shell.execute_reply.started":"2024-10-31T11:56:00.624720Z","shell.execute_reply":"2024-10-31T11:57:45.963853Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Flattening the indices:   0%|          | 0/210904 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3fad02ed6a049619114320161567c2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Flattening the indices:   0%|          | 0/45195 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17ec89cba25a4b6eb6480344e59bbf65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Flattening the indices:   0%|          | 0/45194 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70f85db46d2543dcacd9386b7cecc2a0"}},"metadata":{}}]},{"cell_type":"code","source":"print(ds_1['train'][0:5]['id'])\nprint(ds_2['train'][0:5]['id'])\nprint(ds_concat['train'][0:5]['id'])","metadata":{"execution":{"iopub.status.busy":"2024-10-31T11:59:17.109171Z","iopub.execute_input":"2024-10-31T11:59:17.109574Z","iopub.status.idle":"2024-10-31T11:59:17.119475Z","shell.execute_reply.started":"2024-10-31T11:59:17.109543Z","shell.execute_reply":"2024-10-31T11:59:17.118306Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"['2205.12835', '0706.1875', 'hep-th/0306003', '1307.3106', '1601.01310']\n['hep-ph/0001018', '2111.04548', '1306.4970', '2310.04053', 'hep-ph/0401114']\n['1806.04140', 'hep-th/0209192', 'gr-qc/0505099', 'hep-th/9303053', 'hep-th/9404121']\n","output_type":"stream"}]},{"cell_type":"code","source":"# Push the Dataset to Huggingface\ntry:\n    ds_concat.push_to_hub('LLMsForHepth/hep-th_hep-ph_gr-qc_primary_v3')\nexcept:\n    huggingface_hub.create_repo(repo_id='LLMsForHepth/hep-th_hep-ph_gr-qc_primary_v3',\n                                repo_type=\"dataset\",\n                                private=False)\n    ds_concat.push_to_hub(repo_id)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T11:59:49.829133Z","iopub.execute_input":"2024-10-31T11:59:49.829532Z","iopub.status.idle":"2024-10-31T12:00:10.061167Z","shell.execute_reply.started":"2024-10-31T11:59:49.829501Z","shell.execute_reply":"2024-10-31T12:00:10.060035Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cf9bc7eeb114f6d80dda81a48addc41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Creating parquet from Arrow format:   0%|          | 0/211 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc6b438530bd4bac86b533921be82a9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92377ca592d54bdc95a576f519a44b15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Creating parquet from Arrow format:   0%|          | 0/46 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e504bf6a18a48c79b66f395e655d48f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"414ba7a06828433092290e9cd0b7190e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Creating parquet from Arrow format:   0%|          | 0/46 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98d1a1be93b248dab2835cbab0ca03d0"}},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}