from datasets import load_dataset
import gc
from transformers.integrations import WandbCallback
import torch


class AbstractCompleter:
    def __init__(self, model, tokenizer, dataset, batch_size=None, generation_config=None, col_to_tokenize=None):
        super().__init__()
        self.model = model

        self.tokenizer = tokenizer
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        self.original_pad_side = tokenizer.padding_side

        self.dataset = dataset
        self.batch_size = 16 if batch_size is None else batch_size

        if generation_config is None:
            self.generation_config = {
                "temperature": 0.7,
                "top_p": 0.1,
                "repetition_penalty": 1.18,
                "top_k": 40,
                "do_sample": True,
                "max_new_tokens": 1024,
                "pad_token_id": self.tokenizer.pad_token_id,
                "max_time": 60
            }
        else:
            self.generation_config = generation_config

        if self.generation_config.get("pad_token_id", None) is None:
            self.generation_config.update({"pad_token_id": self.tokenizer.pad_token_id})

        self.label = 'abstract' if col_to_tokenize is None else col_to_tokenize

    def predict(self, batch):
        texts = batch['prompt']
        self.tokenizer.padding_side = 'left'  # inference needs left padding
        # clear some memory
        torch.cuda.empty_cache()
        gc.collect()
        model_inputs = self.tokenizer(texts,
                                      padding='longest',
                                      pad_to_multiple_of=8,
                                      add_special_tokens=False,
                                      return_tensors="pt").to(self.model.device)
        outputs = self.model.generate(**model_inputs, **self.generation_config).to("cpu")
        batch["predictions"] = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)
        self.tokenizer.padding_side = self.original_pad_side
        return batch

    def split_abstracts(self, example):
        """Generates a prompt and ground truth from an abstract.

        The prompt is generated by first splitting the abstract into sentences,
        then selecting at most half of the sentences and recombining them into
        a single paragraph. The ground truth is constructed from the remaining sentences.
        """
        abstract_split = example['abstract'].split('.')  # split abstract into sentences
        prompt_split = abstract_split[:len(abstract_split) // 2]  # choose at most half of the sentences
        y_true_split = abstract_split[len(abstract_split) // 2:]
        prompt = '.'.join(prompt_split) + '.'  # reconstruct the sentences into a paragraph adding a final '.'
        y_true = '.'.join(y_true_split)
        return {'prompt': prompt, 'y_true': y_true}

    def parse_y_pred(self, example):
        """Return the new text from the prediction"""
        len_prompt = len(example['prompt'])
        y_pred = example['predictions'][len_prompt:]
        return {'y_pred': y_pred}

    def get_predictions(self):
        self.dataset = self.dataset.map(self.split_abstracts,
                                        batched=False,
                                        desc='Splitting abstracts')
        self.dataset = self.dataset.map(self.predict,
                                        batched=True,
                                        batch_size=self.batch_size,
                                        desc='Generating model output')
        self.dataset = self.dataset.map(self.parse_y_pred,
                                        batched=False,
                                        desc='Parsing y_pred')
        return self.dataset


class GenCallback(WandbCallback):
    def __init__(self, generation_config=None):
        super().__init__()
        self.generation_config = generation_config
        self.dataset = load_dataset("LLMsForHepth/arxiv_hepth_first_overfit").get('train')
        self.add_prompts = True

    def on_evaluate(self, args, state, control, model=None, tokenizer=None, **kwargs):
        self._gen_and_log(state, model, tokenizer)

    def _gen_and_log(self, state, model, tokenizer):
        switch_back_to_train = False
        if model.training:
            model.eval()
            switch_back_to_train = True
        with torch.no_grad():
            completer = AbstractCompleter(model, tokenizer, dataset=self.dataset, batch_size=5,
                                          generation_config=self.generation_config)
            completions = completer.get_predictions()
            new_table = self._wandb.Table(columns=['global_step', 'abstract 1', 'abstract 2',
                                                   'abstract 3', 'abstract 4', 'abstract 5'])
            if self.add_prompts:
                new_table.add_data("Prompt", *completions['prompt'])
                self.add_prompts = False
            new_table.add_data(str(state.global_step), *completions['predictions'])
            self._wandb.log({f"predictions": new_table}, commit=False)
        if switch_back_to_train:
            model.train()
