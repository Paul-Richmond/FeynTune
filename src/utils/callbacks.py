from datasets import load_dataset
import gc
import re
from transformers.integrations import WandbCallback
import torch


class AbstractCompleter:
    def __init__(self, model, tokenizer, dataset, batch_size=None, generation_config=None, col_to_tokenize=None):
        super().__init__()
        self.model = model

        self.tokenizer = tokenizer
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        self.original_pad_side = tokenizer.padding_side

        self.dataset = dataset
        self.batch_size = 16 if batch_size is None else batch_size

        if generation_config is None:
            self.generation_config = {
                "temperature": 0.7,
                "top_p": 0.1,
                "repetition_penalty": 1.18,
                "top_k": 40,
                "do_sample": True,
                "max_new_tokens": 1024,
                "pad_token_id": self.tokenizer.pad_token_id,
                "max_time": 60
            }
        else:
            self.generation_config = generation_config

        if self.generation_config.get("pad_token_id", None) is None:
            self.generation_config.update({"pad_token_id": self.tokenizer.pad_token_id})

        self.label = 'abstract' if col_to_tokenize is None else col_to_tokenize

    def predict(self, batch):
        texts = batch['prompt']
        self.tokenizer.padding_side = 'left'  # inference needs left padding
        # clear some memory
        torch.cuda.empty_cache()
        gc.collect()
        model_inputs = self.tokenizer(texts,
                                      padding='longest',
                                      pad_to_multiple_of=8,
                                      add_special_tokens=False,
                                      return_tensors="pt").to(self.model.device)
        outputs = self.model.generate(**model_inputs, **self.generation_config).to("cpu")
        batch["predictions"] = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)
        self.tokenizer.padding_side = self.original_pad_side
        return batch

    def split_abstracts(self, example):
        """Generates a prompt and ground truth from an abstract.

        The prompt is generated by first splitting the abstract into sentences,
        then selecting half or just over half of the sentences and recombining them into
        a single paragraph. The ground truth is constructed from the remaining sentences.
        """
        # Split the abstract into sentences (i.e. text sequences which end with any of .!? and a space)
        sentences = re.split(r'(?<=[.!?])\s+', example['abstract'])
        # Calculate the split point
        total_sentences = len(sentences)
        split_point = (total_sentences + 1) // 2  # Ensures the prompt has >= number of sentences than y_true
        # Join the sentences back into two parts
        prompt = ' '.join(sentences[:split_point])
        y_true = ' '.join(sentences[split_point:])
        return {'prompt': prompt, 'y_true': y_true}

    def parse_y_pred(self, example):
        """Return the new text from the prediction"""
        len_prompt = len(example['prompt'])
        y_pred = example['predictions'][len_prompt:]
        return {'y_pred': y_pred}

    def get_predictions(self):
        self.dataset = self.dataset.map(self.split_abstracts,
                                        batched=False,
                                        desc='Splitting abstracts')
        self.dataset = self.dataset.map(self.predict,
                                        batched=True,
                                        batch_size=self.batch_size,
                                        desc='Generating model output')
        self.dataset = self.dataset.map(self.parse_y_pred,
                                        batched=False,
                                        desc='Parsing y_pred')
        return self.dataset


class GenCallback(WandbCallback):
    def __init__(self, generation_config=None):
        super().__init__()
        self.generation_config = generation_config
        self.dataset = load_dataset("LLMsForHepth/arxiv_hepth_first_overfit").get('train')
        self.add_prompts = True

    def on_evaluate(self, args, state, control, model=None, tokenizer=None, **kwargs):
        self._gen_and_log(state, model, tokenizer)

    def _gen_and_log(self, state, model, tokenizer):
        switch_back_to_train = False
        if model.training:
            model.eval()
            switch_back_to_train = True
        with torch.no_grad():
            completer = AbstractCompleter(model, tokenizer, dataset=self.dataset, batch_size=5,
                                          generation_config=self.generation_config)
            completions = completer.get_predictions()
            new_table = self._wandb.Table(columns=['global_step', 'abstract 1', 'abstract 2',
                                                   'abstract 3', 'abstract 4', 'abstract 5'])
            if self.add_prompts:
                new_table.add_data("Prompt", *completions['prompt'])
                self.add_prompts = False
            new_table.add_data(str(state.global_step), *completions['predictions'])
            self._wandb.log({f"predictions": new_table}, commit=False)
        if switch_back_to_train:
            model.train()
