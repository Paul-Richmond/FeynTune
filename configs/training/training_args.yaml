trainer_cfg:
  _target_: transformers.Trainer
  _partial_: true
training_args_cfg:
  _target_: transformers.TrainingArguments
  num_train_epochs: 4
  #max_steps: 100 #uncomment and change value to override num_train_epochs
  eval_strategy: steps
  #If eval_steps smaller than 1, will be interpreted as ratio of total training steps.
  eval_steps: 0.111 #= 1/9, i.e. evaluate 9 times over all epochs
  eval_on_start: true # also evaluate at the start of training so will evaluate 10=1+9 times in total
  save_strategy: steps
  #If save_steps smaller than 1, will be interpreted as ratio of total training steps.
  save_steps: 0.111 #= 1/9, i.e. save 9 times over all epochs
  log_level: info
  logging_steps: 1
  max_grad_norm: 1.0  # bug in Transformers.trainer means unclipped grad_norm value is logged to wandb
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 16
  bf16: true
  output_dir: LLMsForHepth/${training.training_args_cfg.run_name}
  report_to: wandb
  run_name: ???
  push_to_hub: true
  hub_model_id: LLMsForHepth/${training.training_args_cfg.run_name}
  hub_private_repo: true
  hub_strategy: checkpoint # the latest checkpoint is also pushed in a subfolder named last-checkpoint
  lr_scheduler_type: cosine_with_min_lr
  warmup_ratio: 0.1
  lr_scheduler_kwargs:
    min_lr_rate: 0.01
resume_from_checkpoint: null # (str, bool or null), if not null .env file must have WANDB_RESUME & WANDB_RUN_ID set
