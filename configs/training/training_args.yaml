num_train_epochs: 1
evaluation_strategy: "no"
log_level: info
logging_steps: 1
max_grad_norm: 1.0
per_device_train_batch_size: 24
per_device_eval_batch_size: 24
output_dir: hf
report_to: wandb
run_name: ???
push_to_hub: true
hub_model_id: LLMsForHepth/${training.run_name}
hub_private_repo: true
save_steps: 100
lr_scheduler_type: cosine_with_min_lr
warmup_ratio: 0.1
lr_scheduler_kwargs:
  min_lr_rate: 0.1