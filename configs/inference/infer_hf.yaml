model_name: meta-llama/Meta-Llama-3.1-8B

generation_cfg:
  max_new_tokens: 256
  temperature: 0.3
  do_sample: true
  top_p: 0.1
#  max_time: 30

# For outputs
output_dname: s1-L-3.1-8B-base
output_dir: /output_hf/${inference.output_dname}
output_fname: gen_1
