base_model_name: meta-llama/Meta-Llama-3.1-8B
lora_adapter_name: LLMsForHepth/s1-L-3.1-8B-base

#model_cfg:  # see https://docs.vllm.ai/en/latest/dev/offline_inference/llm.html
#  # can also pass any of https://docs.vllm.ai/en/latest/models/engine_args.html#engine-args
#  dtype: auto # default
#  max_num_batched_tokens: 1024 # default is 512
#
generation_cfg:
  # see https://docs.vllm.ai/en/latest/dev/sampling_params.html
  max_tokens: 1024
  min_tokens: 48
  #  temperature: 0.7
  #  repetition_penalty: 1.0
  #  top_k: 100
  #  top_p: 0.5

# see https://docs.wandb.ai/guides/track/public-api-guide#find-the-run-path
#wandb_runpath: ???  # always in the format <entity>/<project>/<run_id>

# For outputs
output_dname: s1-L-3.1-8B-base
output_dir: /data/scratch/${oc.env:USER}/LLMsForHepth_vllm/${inference.output_dname}
output_fname: gen_1