# Base model to fine tune
model_name: "meta-llama/Meta-Llama-3-8B"
# what attention to use
attn_implementation: "flash_attention_2"
# Dictionary of values to use in tokenizing the datasets
tokenizer:
  padding: "max_length"
  max_length: 720
# Dictionary of values to use for loading the datasets
dataset_cfg:
  # Dataset name
  dataset_name: "LLMsForHepth/arxiv_hepth_first"
  # What percentage of the train/test/validation datasets to load
  dataset_percent: ":100%"
# Dictionary of values to use in the torch.optim.AdamW optimizer
optim_cfg:
  betas: !!python/tuple
    - 0.9
    - 0.95
  eps: 1.0e-05
  lr: 0.0003
  weight_decay: 0.1

# Dictionary of values to use in the get_cosine_with_min_lr_schedule_with_warmup learning rate schedule
lr_schedule_cfg:
  # The number of steps for the warmup phase.
  num_warmup_steps: false
  # The total number of training steps.
  num_training_steps: false
  # The number of waves in the cosine schedule (the defaults is to just decrease from the max value to 0 following a half-cosine).
  num_cycles: 0.5
  # The index of the last epoch when resuming training. Default is -1
  last_epoch: -1
  # The minimum learning rate to reach after the cosine schedule.
#  min_lr: none
  # The minimum learning rate as a ratio of the initial learning rate. If set, `min_lr` should not be set.
  min_lr_rate: 0.1

# Dictionary of values to use in Trainer class
training_args_cfg:
  num_train_epochs: 3
  evaluation_strategy: epoch
  log_level: info
  logging_steps: 1
  max_grad_norm: 1.0
  output_dir: hf
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 32
  report_to: wandb
  run_name: my_first_run  #--------CHANGE ME! CHANGE ME! CHANGE ME!--------
  hub_model_id: LLMsForHepth/my_first_run  #--------CHANGE ME! CHANGE ME! CHANGE ME!--------
  hub_private_repo: true
  push_to_hub: false

# Dictionary of values to use setting up model quantization QLoRA
# see https://huggingface.co/docs/transformers/v4.40.1/en/main_classes/quantization#transformers.BitsAndBytesConfig
bnb_cfg:
  bnb_4bit_compute_dtype: bfloat16
  bnb_4bit_quant_storage: null
  bnb_4bit_quant_type: nf4
  bnb_4bit_use_double_quant: true
  load_in_4bit: true

# Dictionary of values to use setting up LoRA
lora_cfg:
  bias: none
  lora_alpha: 32
  lora_dropout: 0.05
  r: 8
  target_modules: # choice taken from https://huggingface.co/blog/mlabonne/orpo-llama-3
    - up_proj
    - down_proj
    - gate_proj
    - k_proj
    - q_proj
    - v_proj
    - o_proj
  task_type: CAUSAL_LM
