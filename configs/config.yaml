# Tokenizer to use
tokenizer_name: "meta-llama/Meta-Llama-3-8B"
# Base model to fine tune
model_name: "meta-llama/Meta-Llama-3-8B"
  # Dictionary of values to use in the AdamW optimizer
dataset_cfg:
  # Dataset name
  dataset_name: "LLMsForHepth/arxiv_hepth_first"
  #
  dataset_percent: ":15%"
optim_cfg:
  betas: !!python/tuple
    - 0.9
    - 0.95
  correct_bias: true
  eps: 1.0e-05
  lr: 0.0003
  no_deprecation_warning: false
  weight_decay: 0.1

lr_schedule_cfg:
# Linear warmup peaks after 10% of optimization steps
  warmup_ratio: 0.1
# Looking at the get_cosine_schedule_with_warmup source code
# it returns max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))
# if we want to achieve 10% of the learning rate on the final step
# then num_cycles = 0.397584 (from Mathematica)
  num_cycles: 0.397584

training_args_cfg:
  evaluation_strategy: epoch
  hub_model_id: LLMsForHepth/hepthLlama3
  hub_private_repo: true
  logging_steps: 1
  max_grad_norm: 1.0
  output_dir: LLMsForHepth/hepthLlama3
  per_device_train_batch_size: 32
  push_to_hub: true
  report_to: wandb
  run_name: Sulis_test

# Dictionary of values to use setting up model quantization QLoRA
# see https://huggingface.co/docs/transformers/v4.40.1/en/main_classes/quantization#transformers.BitsAndBytesConfig
bnb_cfg:
  bnb_4bit_compute_dtype: float16
  bnb_4bit_quant_storage: null
  bnb_4bit_quant_type: nf4
  bnb_4bit_use_double_quant: true
  load_in_4bit: true

# Dictionary of values to use setting up LoRA
lora_cfg:
  bias: none
  lora_alpha: 32
  lora_dropout: 0.05
  r: 8
  target_modules:
    - a_proj
    - v_proj
    - k_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj
    - embed_tokens
    - lm_head
  task_type: CAUSAL_LM
