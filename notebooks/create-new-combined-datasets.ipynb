{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new combined datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import huggingface_hub\n",
    "from datasets import load_dataset, DatasetDict, concatenate_datasets\n",
    "\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    hf_token = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "    huggingface_hub.login(token=hf_token)\n",
    "except:\n",
    "    huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get basic datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_1 = 'LLMsForHepth/hep-th_primary'\n",
    "ds_2 = 'LLMsForHepth/gr-qc_primary'\n",
    "ds_3 = 'LLMsForHepth/hep-ph_primary'\n",
    "ds_4 = 'LLMsForHepth/q-bio_primary'\n",
    "ds_5 = 'LLMsForHepth/cs_primary_200k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_1 = load_dataset(ds_1)\n",
    "ds_2 = load_dataset(ds_2)\n",
    "ds_3 = load_dataset(ds_3)\n",
    "ds_4 = load_dataset(ds_4)\n",
    "ds_5 = load_dataset(ds_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create hep-th + hep-ph and hep-th + gr-qc datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_hep_th_gr_qc = DatasetDict()\n",
    "ds_hep_th_hep_ph = DatasetDict()\n",
    "names = ds_1.keys()\n",
    "\n",
    "for name in names:\n",
    "    ds_hep_th_gr_qc[name] = concatenate_datasets([ds_1[name], ds_2[name]])\n",
    "    ds_hep_th_hep_ph[name] = concatenate_datasets([ds_1[name], ds_3[name]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_hep_th_gr_qc.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_hep_th_hep_ph.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshuffle the data in each split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_hep_th_gr_qc = ds_hep_th_gr_qc.shuffle(seed=42)\n",
    "ds_hep_th_gr_qc = ds_hep_th_gr_qc.flatten_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_hep_th_hep_ph = ds_hep_th_hep_ph.shuffle(seed=42)\n",
    "ds_hep_th_hep_ph = ds_hep_th_hep_ph.flatten_indices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push to Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ds_hep_th_gr_qc.push_to_hub('LLMsForHepth/hep-th_gr-qc_primary')\n",
    "except:\n",
    "    huggingface_hub.create_repo(repo_id='LLMsForHepth/hep-th_gr-qc_primary',\n",
    "                                repo_type=\"dataset\",\n",
    "                                private=False)\n",
    "    ds_hep_th_gr_qc.push_to_hub('LLMsForHepth/hep-th_gr-qc_primary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ds_hep_th_hep_ph.push_to_hub('LLMsForHepth/hep-th_hep-ph_primary')\n",
    "except:\n",
    "    huggingface_hub.create_repo(repo_id='LLMsForHepth/hep-th_hep-ph_primary',\n",
    "                                repo_type=\"dataset\",\n",
    "                                private=False)\n",
    "    ds_hep_th_hep_ph.push_to_hub('LLMsForHepth/hep-th_hep-ph_primary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create combo of gr-qc and hep-ph but keep to same size as hep-th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_hep_th = sum([ds_1[split].num_rows for split in ds_1.keys()])\n",
    "size_gr_qc = sum([ds_2[split].num_rows for split in ds_2.keys()])\n",
    "size_hep_ph = sum([ds_3[split].num_rows for split in ds_3.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop = size_hep_th / (size_gr_qc + size_hep_ph)\n",
    "prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We take prop * gr-qc\n",
    "\n",
    "gr_qc_samp = DatasetDict()\n",
    "for split in ds_2.keys():\n",
    "    gr_qc_samp[split] = ds_2[split].shuffle(seed=42).select(range(int(prop * ds_2[split].num_rows)))\n",
    "\n",
    "# For hep-ph we take enough to make up to the size of hep-th\n",
    "\n",
    "hep_ph_samp = DatasetDict()\n",
    "for split in ds_3.keys():\n",
    "    hep_ph_samp[split] = ds_3[split].shuffle(seed=42).select(range(ds_1[split].num_rows - gr_qc_samp[split].num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The combined sizes are equal to hep-th\n",
    "\n",
    "for split in ds_2.keys():\n",
    "    print(f'Split {split} has size {gr_qc_samp[split].num_rows + hep_ph_samp[split].num_rows}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate gr_qc_samp and hep_ph_samp datasets\n",
    "\n",
    "ds_gr_qc_hep_ph_small = DatasetDict()\n",
    "for split in gr_qc_samp.keys():\n",
    "    ds_gr_qc_hep_ph_small[split] = concatenate_datasets([gr_qc_samp[split], hep_ph_samp[split]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly shuffle the concatenated dataset\n",
    "\n",
    "ds_gr_qc_hep_ph_small = ds_gr_qc_hep_ph_small.shuffle(seed=42)\n",
    "ds_gr_qc_hep_ph_small = ds_gr_qc_hep_ph_small.flatten_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push to Huggingface\n",
    "\n",
    "try:\n",
    "    ds_gr_qc_hep_ph_small.push_to_hub('LLMsForHepth/gr-qc_hep-ph_small')\n",
    "except:\n",
    "    huggingface_hub.create_repo(repo_id='LLMsForHepth/gr-qc_hep-ph_small',\n",
    "                                repo_type=\"dataset\",\n",
    "                                private=False)\n",
    "    ds_gr_qc_hep_ph_small.push_to_hub('LLMsForHepth/gr-qc_hep-ph_small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create hep-th + q-bio + cs dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are creating a new dataset which should have the same size as the combined hep-th + hep-ph + gr-qc\n",
    "wanted_sizes = {split: ds_2[split].num_rows + ds_3[split].num_rows for split in ds_1.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the number of rows we need to select from the cs dataset\n",
    "cs_needed = {split: wanted_sizes[split] - ds_4[split].num_rows for split in wanted_sizes.keys()}\n",
    "cs_needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_samp = DatasetDict()\n",
    "for split in ds_5.keys():\n",
    "    cs_samp[split] = ds_5[split].shuffle(seed=42).select(range(cs_needed[split]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_hep_th_qbio_cs = DatasetDict()\n",
    "for split in ds_1.keys():\n",
    "    ds_hep_th_qbio_cs[split] = concatenate_datasets([ds_1[split], ds_4[split], cs_samp[split]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly shuffle the concatenated dataset\n",
    "\n",
    "ds_hep_th_qbio_cs = ds_hep_th_qbio_cs.shuffle(seed=42)\n",
    "ds_hep_th_qbio_cs = ds_hep_th_qbio_cs.flatten_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push to Huggingface\n",
    "\n",
    "try:\n",
    "    ds_hep_th_qbio_cs.push_to_hub('LLMsForHepth/hep-th_qbio_cs')\n",
    "except:\n",
    "    huggingface_hub.create_repo(repo_id='LLMsForHepth/hep-th_qbio_cs',\n",
    "                                repo_type=\"dataset\",\n",
    "                                private=False)\n",
    "    ds_hep_th_qbio_cs.push_to_hub('LLMsForHepth/hep-th_qbio_cs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_hub.logout()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "llmsforhepth",
   "language": "python",
   "name": "llmsforhepth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
